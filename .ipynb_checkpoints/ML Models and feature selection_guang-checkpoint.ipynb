{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import sklearn.metrics as metrics\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_selection import RFE\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc, roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"new_train.csv\")\n",
    "test = pd.read_csv(\"new_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>bidder_id</th>\n",
       "      <th>payment_account</th>\n",
       "      <th>address</th>\n",
       "      <th>outcome</th>\n",
       "      <th>auction</th>\n",
       "      <th>merchandise</th>\n",
       "      <th>device</th>\n",
       "      <th>time</th>\n",
       "      <th>country</th>\n",
       "      <th>...</th>\n",
       "      <th>max_url_per_auction</th>\n",
       "      <th>min_url_per_auction</th>\n",
       "      <th>std_url_per_auction</th>\n",
       "      <th>total_no_of_participated_auctions</th>\n",
       "      <th>no_of_auction_exceeds_threshold</th>\n",
       "      <th>percentage_of_auctions_above_threshold</th>\n",
       "      <th>total_no_of_bidded_category</th>\n",
       "      <th>no_of_merchandise_exceeds_threshold</th>\n",
       "      <th>percentage_of_merchandise_above_threshold</th>\n",
       "      <th>on_url_that_has_a_bot_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>91a3c57b13234af24875c56fb7e2b2f4rb56a</td>\n",
       "      <td>a3d2de7675556553a5f08e4c88d2c228754av</td>\n",
       "      <td>a3d2de7675556553a5f08e4c88d2c228vt0u4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>624f258b49e77713fc34034560f93fb3hu3jo</td>\n",
       "      <td>a3d2de7675556553a5f08e4c88d2c228v1sga</td>\n",
       "      <td>ae87054e5a97a8f840a3991d12611fdcrfbq3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1c5f4fc669099bfbfac515cd26997bd12ruaj</td>\n",
       "      <td>a3d2de7675556553a5f08e4c88d2c2280cybl</td>\n",
       "      <td>92520288b50f03907041887884ba49c0cl0pd</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4bee9aba2abda51bf43d639013d6efe12iycd</td>\n",
       "      <td>51d80e233f7b6a7dfdee484a3c120f3b2ita8</td>\n",
       "      <td>4cb9717c8ad7e88a9a284989dd79b98dbevyi</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4ab12bc61c82ddd9c2d65e60555808acqgos1</td>\n",
       "      <td>a3d2de7675556553a5f08e4c88d2c22857ddh</td>\n",
       "      <td>2a96c3ce94b3be921e0296097b88b56a7x1ji</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.644263</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>2008</td>\n",
       "      <td>369515b3af4f8ca582f90271d30b14b6r52aw</td>\n",
       "      <td>a1f85275793c4a782f0a668711f41b927ivc9</td>\n",
       "      <td>e6882cf204a9482edd042b6e31791dfctxzx8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>2009</td>\n",
       "      <td>f939c17ffc7c39ac9b35b69e5e75179fv9pe2</td>\n",
       "      <td>a3d2de7675556553a5f08e4c88d2c2286s1m2</td>\n",
       "      <td>b9b03d5a127eb07aeb9163cdcf524e1344ac9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>2010</td>\n",
       "      <td>c806dbb2decba0ed3c4ff5e2e60a74c2wjvbl</td>\n",
       "      <td>a3d2de7675556553a5f08e4c88d2c22856leq</td>\n",
       "      <td>d02c2b288b8aabd79ff47118aff41a2dqwzwc</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>2011</td>\n",
       "      <td>0381a69b7a061e9ace2798fd48f1f537mgq57</td>\n",
       "      <td>fd87037ce0304077079c749f420f0b4c54uo0</td>\n",
       "      <td>f030a221726fbcdfc4dc7dfd1b381a112hieq</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>2012</td>\n",
       "      <td>84a769adc98498f52debfe57b93a0789556f4</td>\n",
       "      <td>fbe0ce34d6546ebd9e4c63afc68b085byd2tf</td>\n",
       "      <td>a3d2de7675556553a5f08e4c88d2c228fib6p</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2013 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                              bidder_id  \\\n",
       "0              0  91a3c57b13234af24875c56fb7e2b2f4rb56a   \n",
       "1              1  624f258b49e77713fc34034560f93fb3hu3jo   \n",
       "2              2  1c5f4fc669099bfbfac515cd26997bd12ruaj   \n",
       "3              3  4bee9aba2abda51bf43d639013d6efe12iycd   \n",
       "4              4  4ab12bc61c82ddd9c2d65e60555808acqgos1   \n",
       "...          ...                                    ...   \n",
       "2008        2008  369515b3af4f8ca582f90271d30b14b6r52aw   \n",
       "2009        2009  f939c17ffc7c39ac9b35b69e5e75179fv9pe2   \n",
       "2010        2010  c806dbb2decba0ed3c4ff5e2e60a74c2wjvbl   \n",
       "2011        2011  0381a69b7a061e9ace2798fd48f1f537mgq57   \n",
       "2012        2012  84a769adc98498f52debfe57b93a0789556f4   \n",
       "\n",
       "                            payment_account  \\\n",
       "0     a3d2de7675556553a5f08e4c88d2c228754av   \n",
       "1     a3d2de7675556553a5f08e4c88d2c228v1sga   \n",
       "2     a3d2de7675556553a5f08e4c88d2c2280cybl   \n",
       "3     51d80e233f7b6a7dfdee484a3c120f3b2ita8   \n",
       "4     a3d2de7675556553a5f08e4c88d2c22857ddh   \n",
       "...                                     ...   \n",
       "2008  a1f85275793c4a782f0a668711f41b927ivc9   \n",
       "2009  a3d2de7675556553a5f08e4c88d2c2286s1m2   \n",
       "2010  a3d2de7675556553a5f08e4c88d2c22856leq   \n",
       "2011  fd87037ce0304077079c749f420f0b4c54uo0   \n",
       "2012  fbe0ce34d6546ebd9e4c63afc68b085byd2tf   \n",
       "\n",
       "                                    address  outcome  auction  merchandise  \\\n",
       "0     a3d2de7675556553a5f08e4c88d2c228vt0u4      0.0     18.0          1.0   \n",
       "1     ae87054e5a97a8f840a3991d12611fdcrfbq3      0.0      1.0          1.0   \n",
       "2     92520288b50f03907041887884ba49c0cl0pd      0.0      4.0          1.0   \n",
       "3     4cb9717c8ad7e88a9a284989dd79b98dbevyi      0.0      1.0          1.0   \n",
       "4     2a96c3ce94b3be921e0296097b88b56a7x1ji      0.0     23.0          1.0   \n",
       "...                                     ...      ...      ...          ...   \n",
       "2008  e6882cf204a9482edd042b6e31791dfctxzx8      0.0     25.0          1.0   \n",
       "2009  b9b03d5a127eb07aeb9163cdcf524e1344ac9      0.0      1.0          1.0   \n",
       "2010  d02c2b288b8aabd79ff47118aff41a2dqwzwc      0.0      1.0          1.0   \n",
       "2011  f030a221726fbcdfc4dc7dfd1b381a112hieq      0.0      1.0          1.0   \n",
       "2012  a3d2de7675556553a5f08e4c88d2c228fib6p      0.0      1.0          1.0   \n",
       "\n",
       "      device   time  country  ...  max_url_per_auction  min_url_per_auction  \\\n",
       "0       14.0   24.0      6.0  ...                  1.0                  1.0   \n",
       "1        2.0    3.0      1.0  ...                  2.0                  2.0   \n",
       "2        2.0    4.0      1.0  ...                  1.0                  1.0   \n",
       "3        1.0    1.0      1.0  ...                  1.0                  1.0   \n",
       "4       53.0  155.0      2.0  ...                 21.0                  1.0   \n",
       "...      ...    ...      ...  ...                  ...                  ...   \n",
       "2008     4.0   33.0      4.0  ...                  1.0                  1.0   \n",
       "2009     1.0    1.0      1.0  ...                  1.0                  1.0   \n",
       "2010     2.0    2.0      1.0  ...                  1.0                  1.0   \n",
       "2011     1.0    1.0      1.0  ...                  1.0                  1.0   \n",
       "2012     1.0    2.0      1.0  ...                  1.0                  1.0   \n",
       "\n",
       "      std_url_per_auction  total_no_of_participated_auctions  \\\n",
       "0                0.000000                               18.0   \n",
       "1                0.000000                                1.0   \n",
       "2                0.000000                                4.0   \n",
       "3                0.000000                                1.0   \n",
       "4                5.644263                               23.0   \n",
       "...                   ...                                ...   \n",
       "2008             0.000000                               25.0   \n",
       "2009             0.000000                                1.0   \n",
       "2010             0.000000                                1.0   \n",
       "2011             0.000000                                1.0   \n",
       "2012             0.000000                                1.0   \n",
       "\n",
       "      no_of_auction_exceeds_threshold  percentage_of_auctions_above_threshold  \\\n",
       "0                                 0.0                                0.000000   \n",
       "1                                 0.0                                0.000000   \n",
       "2                                 0.0                                0.000000   \n",
       "3                                 0.0                                0.000000   \n",
       "4                                 1.0                                0.043478   \n",
       "...                               ...                                     ...   \n",
       "2008                              1.0                                0.040000   \n",
       "2009                              0.0                                0.000000   \n",
       "2010                              0.0                                0.000000   \n",
       "2011                              0.0                                0.000000   \n",
       "2012                              0.0                                0.000000   \n",
       "\n",
       "      total_no_of_bidded_category  no_of_merchandise_exceeds_threshold  \\\n",
       "0                             1.0                                  0.0   \n",
       "1                             1.0                                  0.0   \n",
       "2                             1.0                                  0.0   \n",
       "3                             1.0                                  0.0   \n",
       "4                             1.0                                  0.0   \n",
       "...                           ...                                  ...   \n",
       "2008                          1.0                                  0.0   \n",
       "2009                          1.0                                  0.0   \n",
       "2010                          1.0                                  0.0   \n",
       "2011                          1.0                                  0.0   \n",
       "2012                          1.0                                  0.0   \n",
       "\n",
       "      percentage_of_merchandise_above_threshold  on_url_that_has_a_bot_mean  \n",
       "0                                           0.0                    1.000000  \n",
       "1                                           0.0                    0.500000  \n",
       "2                                           0.0                    0.500000  \n",
       "3                                           0.0                    1.000000  \n",
       "4                                           0.0                    0.010989  \n",
       "...                                         ...                         ...  \n",
       "2008                                        0.0                    0.500000  \n",
       "2009                                        0.0                    0.000000  \n",
       "2010                                        0.0                    0.000000  \n",
       "2011                                        0.0                    0.000000  \n",
       "2012                                        0.0                    0.000000  \n",
       "\n",
       "[2013 rows x 58 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(train.filter(regex=\"Unname\"),axis=1, inplace=True)\n",
    "test.drop(test.filter(regex=\"Unname\"),axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(columns=['bidder_id', 'payment_account', 'address', 'outcome','merchandise']) \n",
    "y = train['outcome']\n",
    "X_test = test.drop(columns=['bidder_id', 'payment_account', 'address', 'merchandise'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['bidder_id', 'payment_account', 'address', 'outcome', 'auction',\n",
       "       'merchandise', 'device', 'time', 'country', 'ip', 'url', 'num_bids',\n",
       "       'num_first_bids', 'num_last_bids', 'time_to_bid', 'inst_resp',\n",
       "       'perc_inst_resp', 'auto parts', 'books and music', 'clothing',\n",
       "       'computers', 'furniture', 'home goods', 'jewelry', 'mobile',\n",
       "       'office equipment', 'sporting goods', 'num_bids_per_auction',\n",
       "       'num_bids_per_device', 'num_bids_per_country', 'num_bids_per_ip',\n",
       "       'on_ip_that_has_a_bot_mean', 'ip_entropy', 'url_entropy',\n",
       "       'mean_country_per_auction', 'max_country_per_auction',\n",
       "       'min_country_per_auction', 'std_country_per_auction',\n",
       "       'mean_devices_per_auction', 'max_devices_per_auction',\n",
       "       'min_devices_per_auction', 'std_devices_per_auction',\n",
       "       'mean_ip_per_auction', 'max_ip_per_auction', 'min_ip_per_auction',\n",
       "       'std_ip_per_auction', 'mean_url_per_auction', 'max_url_per_auction',\n",
       "       'min_url_per_auction', 'std_url_per_auction',\n",
       "       'total_no_of_participated_auctions', 'no_of_auction_exceeds_threshold',\n",
       "       'percentage_of_auctions_above_threshold', 'total_no_of_bidded_category',\n",
       "       'no_of_merchandise_exceeds_threshold',\n",
       "       'percentage_of_merchandise_above_threshold',\n",
       "       'on_url_that_has_a_bot_mean'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the training and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "# scaled_features = data.copy()\n",
    "col_names = ['auction', 'device', 'time', 'country', 'ip', 'url', 'num_bids',\n",
    "       'num_first_bids', 'num_last_bids', 'time_to_bid', 'inst_resp',\n",
    "       'perc_inst_resp', 'num_bids_per_auction',\n",
    "       'num_bids_per_device', 'num_bids_per_country', 'num_bids_per_ip',\n",
    "       'on_ip_that_has_a_bot_mean', 'ip_entropy', 'url_entropy',\n",
    "       'mean_country_per_auction', 'max_country_per_auction',\n",
    "       'min_country_per_auction', 'std_country_per_auction',\n",
    "       'mean_devices_per_auction', 'max_devices_per_auction',\n",
    "       'min_devices_per_auction', 'std_devices_per_auction',\n",
    "       'mean_ip_per_auction', 'max_ip_per_auction', 'min_ip_per_auction',\n",
    "       'std_ip_per_auction', 'mean_url_per_auction', 'max_url_per_auction',\n",
    "       'min_url_per_auction', 'std_url_per_auction',\n",
    "       'total_no_of_participated_auctions', 'no_of_auction_exceeds_threshold',\n",
    "       'percentage_of_auctions_above_threshold', 'total_no_of_bidded_category',\n",
    "       'no_of_merchandise_exceeds_threshold',\n",
    "       'percentage_of_merchandise_above_threshold',\n",
    "       'on_url_that_has_a_bot_mean']\n",
    "features = X[col_names]\n",
    "scaler = StandardScaler().fit(features.values)\n",
    "features = scaler.transform(features.values)\n",
    "X[col_names] = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = StandardScaler()\n",
    "features = X_test[col_names]\n",
    "scaler_test = StandardScaler().fit(features.values)\n",
    "features = scaler_test.transform(features.values)\n",
    "X_test[col_names] = features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying SMOTE to the Scaled Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(sampling_strategy='minority', random_state = 42)\n",
    "X, y = sm.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deciding on features to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # trying selectkbest\n",
    "# selector = SelectKBest(mutual_info_regression, k = 20)\n",
    "# selector.fit(X,y)\n",
    "# selectkbest_support = selector.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # trying rfe\n",
    "# rfe_random_forest = RFE(estimator=RandomForestClassifier(), n_features_to_select = 20, step = 1)\n",
    "# rfe_random_forest.fit(X,y)\n",
    "# rfe_random_forest_support = rfe_random_forest.get_support()\n",
    "\n",
    "# rfe_xgb = RFE(estimator=XGBClassifier(), n_features_to_select = 20, step = 1)\n",
    "# rfe_xgb.fit(X,y)\n",
    "# rfe_xgb_support = rfe_xgb.get_support()\n",
    "\n",
    "# rfe_gb = RFE(estimator=GradientBoostingClassifier(), n_features_to_select = 20, step = 1)\n",
    "# rfe_gb.fit(X,y)\n",
    "# rfe_gb_support = rfe_gb.get_support()\n",
    "\n",
    "# rfe_logistic = RFE(estimator=LogisticRegression(), n_features_to_select = 20, step = 1)\n",
    "# rfe_logistic.fit(X,y)\n",
    "# rfe_logistic_support = rfe_logistic.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # trying selectfrommodel\n",
    "# sfm_random_forest = SelectFromModel(RandomForestClassifier(n_estimators=100), max_features=20)\n",
    "# sfm_random_forest.fit(X,y)\n",
    "# sfm_random_forest_support = sfm_random_forest.get_support()\n",
    "\n",
    "# sfm_xgb = SelectFromModel(XGBClassifier(), max_features=20)\n",
    "# sfm_xgb.fit(X,y)\n",
    "# sfm_xgb_support = sfm_xgb.get_support()\n",
    "\n",
    "# sfm_gb = SelectFromModel(GradientBoostingClassifier(n_estimators=100), max_features=20)\n",
    "# sfm_gb.fit(X,y)\n",
    "# sfm_gb_support = sfm_gb.get_support()\n",
    "\n",
    "# # sfm_logistic = SelectFromModel(LogisticRegression(), max_features=20)\n",
    "# # sfm_logistic.fit(X,y)\n",
    "# # sfm_logistic_support = sfm_logistic.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # https://towardsdatascience.com/the-5-feature-selection-algorithms-every-data-scientist-need-to-know-3a6b566efd2\n",
    "# feature_selection_df = pd.DataFrame({'Feature':X.columns, 'SelectKBest':selectkbest_support, \n",
    "#                                      'RFE Random Forest':rfe_random_forest_support, \n",
    "#                                      'RFE XGB':rfe_xgb_support, 'RFE GB':rfe_gb_support,\n",
    "#                                      'RFE Logistic Regression':rfe_logistic_support, \n",
    "#                                      'SFM Random Forest':sfm_random_forest_support, 'SFM XGB':sfm_xgb_support,\n",
    "#                                      'SFM GB':sfm_gb_support})\n",
    "# # count the selected times for each feature\n",
    "# feature_selection_df['Total'] = np.sum(feature_selection_df, axis=1)\n",
    "# # display the top 100\n",
    "# feature_selection_df = feature_selection_df.sort_values(['Total','Feature'] , ascending=False)\n",
    "# feature_selection_df.index = range(1, len(feature_selection_df)+1)\n",
    "# # feature_selection_df.head(num_feats)\n",
    "# feature_selection_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features =['num_bids_per_device',\n",
    " 'percentage_of_auctions_above_threshold',\n",
    " 'time_to_bid',\n",
    " 'num_bids_per_ip',\n",
    " 'on_url_that_has_a_bot_mean',\n",
    " 'num_bids_per_country',\n",
    " 'auction',\n",
    " 'max_devices_per_auction',\n",
    " 'num_last_bids',\n",
    " 'mobile',\n",
    " 'no_of_auction_exceeds_threshold',\n",
    " 'num_first_bids',\n",
    " 'ip',\n",
    " 'url',\n",
    " 'jewelry',\n",
    " 'num_bids_per_auction',\n",
    " 'max_ip_per_auction',\n",
    " 'inst_resp',\n",
    " 'computers',\n",
    " 'url_entropy']\n",
    "X = X[selected_features]\n",
    "X_test = X_test[selected_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traaining all models on using train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brennan's model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model(model, num_splits, X, y):\n",
    "    skfolds = StratifiedKFold(n_splits=num_splits)\n",
    "    for train_index, test_index in skfolds.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "#         X_train, X_test = X[train_index], X[test_index]\n",
    "#         y_train, y_test = y[train_index], y[test_index]        \n",
    "    \n",
    "        sm = SMOTE()\n",
    "        X_train_oversampled, y_train_oversampled = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "        \n",
    "        model = model\n",
    "        model.fit(X_train_oversampled, y_train_oversampled )  \n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        print(f'Accuracy: {model.score(X_test, y_test)}')\n",
    "        print(f'f-score: {f1_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from catboost import CatBoostClassifier\n",
    "model = CatBoostClassifier(learning_rate=0.1, max_depth=3,random_state = 10)\n",
    "num_splits = 10\n",
    "training_model(model, num_splits, X_train, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pycaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycaret\n",
    "from pycaret.datasets import get_data\n",
    "data = X_train\n",
    "from pycaret.classification import *\n",
    "exp_name = setup(data = new_train_df,target = 'outcome')\n",
    "best_model = compare_models()\n",
    "best_model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=3, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_features, test_labels):\n",
    "    predictions = model.predict_proba(test_features)\n",
    "    pred = model.predict(test_features)\n",
    "    \n",
    "    accuracy = accuracy_score(test_labels, pred)\n",
    "    auc_roc_score = roc_auc_score(test_labels,predictions[:,1])\n",
    "    \n",
    "    print(\"Classification report\")\n",
    "    print(classification_report(test_labels, pred, digits = 4))\n",
    "    \n",
    "    print(\"FBeta Score\")\n",
    "    print(fbeta_score(test_labels, pred, average='binary', beta=2.0))\n",
    "    \n",
    "    print('Model Performance')\n",
    "    print('Accuracy = {:0.4f}%.'.format(accuracy))\n",
    "    print('AUC ROC = {:0.4f}%.'.format(auc_roc_score))\n",
    "    print(\"*\" * 100)\n",
    "    \n",
    "    return accuracy, auc_roc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brennanszeto/Environments/dsenv/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:35:21] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-05 02:35:21.555662: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-05 02:35:21.674943: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9674    0.9338    0.9503       604\n",
      "         1.0     0.9290    0.9649    0.9466       542\n",
      "\n",
      "    accuracy                         0.9485      1146\n",
      "   macro avg     0.9482    0.9494    0.9485      1146\n",
      "weighted avg     0.9492    0.9485    0.9486      1146\n",
      "\n",
      "FBeta Score\n",
      "0.9575247162211644\n",
      "Model Performance\n",
      "Accuracy = 0.9485%.\n",
      "AUC ROC = 0.9494%.\n",
      "****************************************************************************************************\n",
      "Classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9880    0.9503    0.9688       604\n",
      "         1.0     0.9469    0.9871    0.9666       542\n",
      "\n",
      "    accuracy                         0.9677      1146\n",
      "   macro avg     0.9674    0.9687    0.9677      1146\n",
      "weighted avg     0.9685    0.9677    0.9677      1146\n",
      "\n",
      "FBeta Score\n",
      "0.9787778997438711\n",
      "Model Performance\n",
      "Accuracy = 0.9677%.\n",
      "AUC ROC = 0.9952%.\n",
      "****************************************************************************************************\n",
      "Classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9881    0.9619    0.9748       604\n",
      "         1.0     0.9588    0.9871    0.9727       542\n",
      "\n",
      "    accuracy                         0.9738      1146\n",
      "   macro avg     0.9734    0.9745    0.9738      1146\n",
      "weighted avg     0.9742    0.9738    0.9738      1146\n",
      "\n",
      "FBeta Score\n",
      "0.9812912692589875\n",
      "Model Performance\n",
      "Accuracy = 0.9738%.\n",
      "AUC ROC = 0.9971%.\n",
      "****************************************************************************************************\n",
      "Classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7807    0.6954    0.7356       604\n",
      "         1.0     0.6974    0.7823    0.7374       542\n",
      "\n",
      "    accuracy                         0.7365      1146\n",
      "   macro avg     0.7390    0.7388    0.7365      1146\n",
      "weighted avg     0.7413    0.7365    0.7364      1146\n",
      "\n",
      "FBeta Score\n",
      "0.7636887608069163\n",
      "Model Performance\n",
      "Accuracy = 0.7365%.\n",
      "AUC ROC = 0.7401%.\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "def create_model(learn_rate=0.01, momentum=0):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "xgb = XGBClassifier(random_state=42)\n",
    "ann = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "dt.fit(X_train, y_train)\n",
    "rf.fit(X_train, y_train)\n",
    "xgb.fit(X_train, y_train)\n",
    "ann.fit(X_train, y_train)\n",
    "\n",
    "dt_accuracy = evaluate(dt, X_test, y_test)\n",
    "rf_accuracy = evaluate(rf, X_test, y_test)\n",
    "xgb_accuracy = evaluate(xgb, X_test, y_test)\n",
    "ann_accuracy = evaluate(ann, X_test, y_test)\n",
    "\n",
    "\n",
    "# print(\"dt base model search results: \" + str(dt_accuracy))\n",
    "# print(\"*\" * 100)\n",
    "# print(\"rf base model search results: \" + str(rf_accuracy))\n",
    "# print(\"*\" * 100)\n",
    "# print(\"xgb base model search results: \" + str(xgb_accuracy))\n",
    "# print(\"*\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for DT\n",
    "criterion = ['gini', 'entropy']\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [1,2,5,8]\n",
    "min_samples_leaf = [1,11,21,31]\n",
    "max_features = [5,10,15,25]\n",
    "min_impurity_decrease = [0.00005,0.0005,0.005,0.05]\n",
    "\n",
    "dt_random_grid = {'criterion': criterion,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'max_features': max_features,\n",
    "               'min_samples_leaf': min_samples_leaf, \n",
    "               'min_impurity_decrease': min_impurity_decrease}\n",
    "\n",
    "# parameters for Random Forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "max_features = ['auto', 'sqrt',5,10,25]\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4,8,10]\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "rf_random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "\n",
    "\n",
    "base_model = XGBClassifier()\n",
    "# parameters for XGB\n",
    "xgb_random_grid = {'min_child_weight': range(1,10),\n",
    "                    'gamma': [i/10.0 for i in range(0,10)],\n",
    "                    'subsample': [i/10.0 for i in range(3,10)],\n",
    "                    'colsample_bytree': [i/10.0 for i in range(6,10)],\n",
    "                    'max_depth': [3, 4, 5]}\n",
    "\n",
    "def create_model(learn_rate=0.01, momentum=0):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# define the grid search parameters\n",
    "learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "batch_size = [10, 20, 40, 60, 80, 100]\n",
    "epochs = [10, 50, 100]\n",
    "\n",
    "ann_random_grid = dict(learn_rate=learn_rate,momentum=momentum,batch_size=batch_size, epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 100 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brennanszeto/Environments/dsenv/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "86 fits failed out of a total of 200.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "62 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/brennanszeto/Environments/dsenv/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/brennanszeto/Environments/dsenv/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"/Users/brennanszeto/Environments/dsenv/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 250, in fit\n",
      "    raise ValueError(\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "24 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/brennanszeto/Environments/dsenv/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/brennanszeto/Environments/dsenv/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"/Users/brennanszeto/Environments/dsenv/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 308, in fit\n",
      "    raise ValueError(\"max_features must be in (0, n_features]\")\n",
      "ValueError: max_features must be in (0, n_features]\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/brennanszeto/Environments/dsenv/lib/python3.9/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.94156748 0.82931231        nan 0.91281132 0.94265726 0.94146953\n",
      " 0.92987773        nan 0.93753694        nan 0.94156748 0.93713898\n",
      " 0.93083038        nan        nan        nan        nan 0.90372594\n",
      "        nan        nan 0.90115851        nan 0.89671433 0.8899546\n",
      " 0.92775584 0.88758922        nan        nan        nan        nan\n",
      " 0.91957721        nan        nan 0.84406876 0.92107278 0.92915794\n",
      "        nan        nan        nan        nan 0.93327411        nan\n",
      "        nan 0.93388589 0.91071572 0.85531689 0.93154458        nan\n",
      " 0.91957721 0.82931231 0.92742728 0.82931231 0.93607271 0.92775584\n",
      "        nan 0.92965832        nan        nan        nan        nan\n",
      "        nan 0.8348804  0.94156748        nan        nan 0.84406876\n",
      "        nan 0.92107278        nan        nan        nan 0.94013908\n",
      " 0.89943905 0.93083038 0.93284201 0.90321044 0.91281132 0.84359804\n",
      "        nan        nan 0.93393179        nan 0.84406876 0.88814614\n",
      " 0.94156748        nan        nan        nan 0.94741992        nan\n",
      "        nan 0.93373812 0.93083038 0.94003217 0.93607271 0.91957721\n",
      " 0.92987773 0.94757664 0.93538146 0.92107278]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 100 candidates, totalling 200 fits\n"
     ]
    }
   ],
   "source": [
    "# models\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "xgb = XGBClassifier(random_state=42)\n",
    "ann = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# declaring params\n",
    "dt_random = RandomizedSearchCV(estimator = dt, param_distributions = dt_random_grid, n_iter = 100, cv = 2, \n",
    "                               verbose=2, random_state=42, n_jobs = -1, scoring='roc_auc')\n",
    "\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = rf_random_grid, n_iter = 100, cv = 2, verbose=2, \n",
    "                               random_state=42, n_jobs = -1, scoring='roc_auc')\n",
    "\n",
    "xgb_random = RandomizedSearchCV(estimator = xgb, param_distributions = xgb_random_grid, n_iter=100, scoring='roc_auc', \n",
    "                                n_jobs=-1, cv=2, random_state = 42)\n",
    "\n",
    "ann_random = RandomizedSearchCV(estimator = ann, param_distributions = ann_random_grid, n_iter = 100, cv = 2, \n",
    "                                verbose=2, random_state=42, n_jobs = -1, scoring='roc_auc')\n",
    "\n",
    "\n",
    "# # train the model\n",
    "dt_random.fit(X_train, y_train)\n",
    "rf_random.fit(X_train, y_train)\n",
    "xgb_random.fit(X_train, y_train)\n",
    "ann_random.fit(X_train, y_train)\n",
    "\n",
    "# # evaluate\n",
    "dt_best_random = dt_random.best_estimator_\n",
    "dt_random_accuracy = evaluate(dt_best_random, X_test, y_test)\n",
    "\n",
    "rf_best_random = rf_random.best_estimator_\n",
    "rf_random_accuracy = evaluate(rf_best_random, X_test, y_test)\n",
    "\n",
    "xgb_best_random = xgb_random.best_estimator_\n",
    "xgb_random_accuracy = evaluate(xgb_best_random, X_test, y_test)\n",
    "\n",
    "ann_best_random = ann_random.best_estimator_\n",
    "ann_random_accuracy = evaluate(ann_best_random, X_test, y_test)\n",
    "\n",
    "# # print(\"dt random search results: \" + str(dt_random_accuracy))\n",
    "# # print(\"*\" * 100)\n",
    "# # print(\"rf random search results: \" + str(rf_random_accuracy))\n",
    "# # print(\"*\" * 100)\n",
    "# # print(\"xgb random search results: \" + str(xgb_random_accuracy))\n",
    "# # print(\"*\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for DT\n",
    "dt_grid = {'criterion': ['gini'],\n",
    "    'max_depth': [47, 50, 53],\n",
    "#     'max_features': [23, 25, 27],\n",
    "    'min_impurity_decrease': [0.001, 0.004, 0.007],\n",
    "    'min_samples_leaf': [1, 2, 3],\n",
    "    'min_samples_split': [6, 8, 10]\n",
    "}\n",
    "\n",
    "# parameters for Random Forest\n",
    "rf_grid = {\n",
    "    'bootstrap': [False],\n",
    "    'max_depth': [20,30, 40],\n",
    "    'max_features': [5,8,10],\n",
    "    'min_samples_leaf': [1,2,3, 4],\n",
    "    'min_samples_split': [1,3, 5],\n",
    "    'n_estimators': [1200,1300,1500,2000]\n",
    "}\n",
    "\n",
    "\n",
    "# parameters for XGB\n",
    "xgb_grid = {\n",
    "    'max_depth':[2,3,4,5,6,7],\n",
    "    'min_child_weight':[3,5,7,9],\n",
    "    'gamma':[i/10.0 for i in range(0,10)],\n",
    "    'subsample':[i/10.0 for i in range(4,10)],\n",
    "    'colsample_bytree':[i/10.0 for i in range(4,10)]\n",
    "}\n",
    "\n",
    "def create_model(learn_rate=0.01, momentum=0):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "learn_rate = [0.01, 0.1, 0.2]\n",
    "momentum = [0.0, 0.2, 0.4]\n",
    "batch_size = [5, 10, 20]\n",
    "epochs = [50, 100, 150]\n",
    "\n",
    "ann_grid = dict(learn_rate=learn_rate,momentum=momentum,batch_size=batch_size, epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "xgb = XGBClassifier(random_state=42)\n",
    "ann = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# declaring params\n",
    "dt_grid = GridSearchCV(estimator = dt, param_grid = dt_grid, cv = 2, n_jobs = -1, verbose = 2, scoring='roc_auc')\n",
    "rf_grid = GridSearchCV(estimator = rf, param_grid = rf_grid, cv = 2, n_jobs = -1, verbose = 2, scoring='roc_auc')\n",
    "xgb_grid = GridSearchCV(estimator = xgb, param_grid = xgb_grid, scoring='roc_auc',n_jobs=-1, cv=2)\n",
    "ann_grid = GridSearchCV(estimator = ann, param_grid = ann_grid, n_jobs=-1, cv=2, scoring='roc_auc')\n",
    "\n",
    "# train the model\n",
    "dt_grid.fit(X_train, y_train)\n",
    "rf_grid.fit(X_train, y_train)\n",
    "xgb_grid.fit(X_train, y_train)\n",
    "ann_grid.fit(X_train, y_train)\n",
    "\n",
    "# evaluate\n",
    "dt_best_grid = dt_grid.best_estimator_\n",
    "dt_grid_accuracy = evaluate(dt_best_grid, X_test, y_test)\n",
    "\n",
    "rf_best_grid = rf_grid.best_estimator_\n",
    "rf_grid_accuracy = evaluate(rf_best_grid, X_test, y_test)\n",
    "\n",
    "xgb_best_grid = xgb_grid.best_estimator_\n",
    "xgb_grid_accuracy = evaluate(xgb_best_grid, X_test, y_test)\n",
    "\n",
    "ann_best_grid = ann_grid.best_estimator_\n",
    "ann_grid_accuracy = evaluate(ann_best_grid, X_test, y_test)\n",
    "\n",
    "# print(\"dt random search results: \" + str(dt_grid_accuracy))\n",
    "# print(\"*\" * 100)\n",
    "# print(\"rf random search results: \" + str(rf_grid_accuracy))\n",
    "# print(\"*\" * 100)\n",
    "# print(\"xgb random search results: \" + str(xgb_grid_accuracy))\n",
    "# print(\"*\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing Feature Selection with RFE and RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max_features must be in (0, n_features]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-e8c95dcea855>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mrf_selector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRFECV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrf_best_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mrf_selector_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrf_selector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mrf_selector_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrf_selector_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msupport_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\feature_selection\\_rfe.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdelayed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_rfe_single_fit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m         scores = parallel(\n\u001b[0m\u001b[0;32m    551\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrfe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             for train, test in cv.split(X, y, groups))\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\feature_selection\\_rfe.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m         scores = parallel(\n\u001b[1;32m--> 551\u001b[1;33m             \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrfe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    552\u001b[0m             for train, test in cv.split(X, y, groups))\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\feature_selection\\_rfe.py\u001b[0m in \u001b[0;36m_rfe_single_fit\u001b[1;34m(rfe, estimator, X, y, train, test, scorer)\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_safe_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_safe_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     return rfe._fit(\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         _score(estimator, X_test[:, features], y_test, scorer)).scores_\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\feature_selection\\_rfe.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, step_score)\u001b[0m\n\u001b[0;32m    194\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Fitting estimator with %d features.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msupport_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m             \u001b[1;31m# Get coefs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    384\u001b[0m             \u001b[1;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m             \u001b[1;31m# since correctness does not rely on using threads.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n\u001b[0m\u001b[0;32m    387\u001b[0m                              \u001b[1;33m**\u001b[0m\u001b[0m_joblib_parallel_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'threads'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m                 delayed(_parallel_build_trees)(\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1027\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1029\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1030\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    845\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 847\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    848\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    763\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 765\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    766\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    250\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    253\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    250\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    253\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    888\u001b[0m         \"\"\"\n\u001b[0;32m    889\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 890\u001b[1;33m         super().fit(\n\u001b[0m\u001b[0;32m    891\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    892\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    277\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"max_depth must be greater than zero. \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mmax_features\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_features_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 279\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"max_features must be in (0, n_features]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    280\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_leaf_nodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIntegral\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m             raise ValueError(\"max_leaf_nodes must be integral number but was \"\n",
      "\u001b[1;31mValueError\u001b[0m: max_features must be in (0, n_features]"
     ]
    }
   ],
   "source": [
    "# dt_best_grid\n",
    "# rf_best_grid\n",
    "# xgb_best_grid\n",
    "dt_selector = RFECV(dt_best_grid, step=1, cv=5)\n",
    "dt_selector_results = dt_selector.fit(X, y)\n",
    "dt_selector_features = dt_selector_results.support_\n",
    "\n",
    "rf_selector = RFECV(rf_best_grid, step=1, cv=5)\n",
    "rf_selector_results = rf_selector.fit(X, y)\n",
    "rf_selector_features = rf_selector_results.support_\n",
    "\n",
    "xgb_selector = RFECV(xgb_best_grid, step=1, cv=5)\n",
    "xgb_selector_results = xgb_selector.fit(X, y)\n",
    "xgb_selector_features = xgb_selector_results.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_feature_importances = dt_best_grid.feature_importances_\n",
    "dt_df = pd.DataFrame({\"features\": X.columns, \"features_weights\": dt_feature_importances})\n",
    "dt_df.sort_values(by='features_weights', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rf_selector_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-279d2e41f536>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m rfeCV_feat = {\"features\": X.columns, \"Decision Tree\": dt_selector_features, \"Random Forest\": rf_selector_features, \n\u001b[0m\u001b[0;32m      2\u001b[0m               \"XGBoost\": xgb_selector_features}\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mfeatures_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrfeCV_feat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mfeatures_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rf_selector_features' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = RandomForestClassifier(n_estimators=500, max_features=5,\n",
    "                                   max_depth=8, random_state=20,\n",
    "                                   criterion='entropy',)\n",
    "\n",
    "model2 = XGBClassifier( objective= 'binary:logistic', nthread= 10,\n",
    "                              eval_metric= 'auc', silent= 1, seed= 20,\n",
    "\n",
    "                              max_depth= 6, gamma= 0, base_score= 0.50,\n",
    "                              min_child_weight= 4, subsample= 0.5,\n",
    "                              colsample_bytree= 1, eta= 0.01,)\n",
    "\n",
    "model3 = GradientBoostingClassifier(n_estimators=200,\n",
    "                                       random_state=20,\n",
    "                                       max_depth=5,\n",
    "                                       learning_rate=0.03,\n",
    "                                       max_features=5,)\n",
    "model4 = CatBoostClassifier(random_state=20)\n",
    "model5 = DecisionTreeClassifier(criterion = 'gini', max_depth = 50, max_features = 23, \n",
    "                                min_impurity_decrease = 0.001, min_samples_leaf = 1,\n",
    "                                min_samples_split = 8)\n",
    "\n",
    "k_fold_accuracy = []\n",
    "kf = KFold(n_splits=10,shuffle = True)\n",
    "\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    X_train_new, X_test_new = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train_new, y_test_new = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model1.fit(X_train_new,y_train_new)\n",
    "    model2.fit(X_train_new,y_train_new)\n",
    "    model3.fit(X_train_new,y_train_new)\n",
    "    model4.fit(X_train_new,y_train_new)\n",
    "    model5.fit(X_train_new,y_train_new)\n",
    "\n",
    "\n",
    "    y_pred1 = model1.predict_proba(X_test_new)\n",
    "    y_pred2 = model2.predict_proba(X_test_new)\n",
    "    y_pred3 = model3.predict_proba(X_test_new)\n",
    "    y_pred4 = model4.predict_proba(X_test_new)\n",
    "    y_pred5 = model5.predict_proba(X_test_new)\n",
    "\n",
    "\n",
    "    accu1 = model1.predict(X_test_new)\n",
    "    accu2 = model2.predict(X_test_new)\n",
    "    accu3 = model3.predict(X_test_new)\n",
    "    accu4 = model4.predict(X_test_new)\n",
    "    accu5 = model5.predict(X_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nAccuracy_score on RandomForest : ', accuracy_score(y_test_new, accu1))\n",
    "print('\\nAccuracy_score on XGB : ', accuracy_score(y_test_new, accu2))\n",
    "print('\\nAccuracy_score on GradientBoost : ', accuracy_score(y_test_new, accu3))\n",
    "print('\\nAccuracy_score on CatBoost : ', accuracy_score(y_test_new, accu4))\n",
    "\n",
    "averaging_accuracy = (accuracy_score(y_test_new, accu1) + accuracy_score(y_test_new, accu2) +accuracy_score(y_test_new, accu3)+accuracy_score(y_test_new, accu4))/4\n",
    "print('\\nAverage Accuracy : ', averaging_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AUC SCORES\")\n",
    "print(\"-----------------------------------\")\n",
    "print(\"RandomForest : \" , roc_auc_score(y_test_new,y_pred1[:,1]))\n",
    "\n",
    "print(\"XGB : \" , roc_auc_score(y_test_new,y_pred2[:,1]))\n",
    "print(\"Gradient Boost : \" , roc_auc_score(y_test_new,y_pred3[:,1]))\n",
    "print(\"Cat Boost : \", roc_auc_score(y_test_new,y_pred4[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmodel1 = RandomForestClassifier(n_estimators=500, max_features=5,\n",
    "                                   max_depth=8, random_state=20,\n",
    "                                   criterion='entropy',)\n",
    "\n",
    "fmodel2 = XGBClassifier( objective= 'binary:logistic', nthread= 10,\n",
    "                              eval_metric= 'auc', silent= 1, seed= 20,\n",
    "\n",
    "                              max_depth= 6, gamma= 0, base_score= 0.50,\n",
    "                              min_child_weight= 4, subsample= 0.5,\n",
    "                              colsample_bytree= 1, eta= 0.01,)\n",
    "\n",
    "fmodel3 = GradientBoostingClassifier(n_estimators=200,\n",
    "                                       random_state=20,\n",
    "                                       max_depth=5,\n",
    "                                       learning_rate=0.03,\n",
    "                                       max_features=5,)\n",
    "fmodel4 = CatBoostClassifier(random_state=20)\n",
    "\n",
    "\n",
    "fmodel1=fmodel1.fit(X_train, y)\n",
    "fmodel2=fmodel2.fit(X_train, y)\n",
    "fmodel3=fmodel3.fit(X_train, y)\n",
    "fmodel4=fmodel4.fit(X_train, y)\n",
    "\n",
    "final_answer1= fmodel1.predict_proba(X_test)[:,1]\n",
    "final_answer2= fmodel2.predict_proba(X_test)[:,1]\n",
    "final_answer3= fmodel3.predict_proba(X_test)[:,1]\n",
    "final_answer4= fmodel4.predict_proba(X_test)[:,1]\n",
    "\n",
    "\n",
    "finalpredd= (final_answer1+final_answer2+final_answer3+final_answer4)/4\n",
    "print(len(finalpredd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(finalpredd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df= pd.DataFrame({\n",
    "    'bidder_id': test.bidder_id,\n",
    "    'Prediction':finalpredd\n",
    "})\n",
    "\n",
    "output_df.to_csv('facebooksubmission.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = shap.TreeExplainer(fmodel1).shap_values(X)\n",
    "shap.summary_plot(shap_values, X, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
