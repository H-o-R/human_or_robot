{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as  pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# import xgboost as xgb\n",
    "# from catboost import CatBoostClassifier      \n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "# from sklearn.model_selection import KFold\n",
    "# import statistics\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature engineered dfs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_df = pd.read_csv('new_train.csv')\n",
    "new_test_df = pd.read_csv('new_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# sns.countplot(new_train_df['outcome'],label=\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_train_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# corr = new_train_df.iloc[:,:].corr()\n",
    "# colormap = sns.diverging_palette(220, 10, as_cmap = True)\n",
    "# plt.figure(figsize=(14,14))\n",
    "# sns.heatmap(corr, cbar = True,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 12},\n",
    "#             cmap = colormap, linewidths=0.1, linecolor='white')\n",
    "# plt.title('Correlation of new_train Features', y=1.05, size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified K-Fold with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def training_model(model, num_splits, X, y):\n",
    "    skfolds = StratifiedKFold(n_splits=num_splits)\n",
    "\n",
    "    for train_index, test_index in skfolds.split(X, y):\n",
    "#         X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "#         y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]        \n",
    "    \n",
    "        sm = SMOTE()\n",
    "        X_train_oversampled, y_train_oversampled = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "        \n",
    "        model = model\n",
    "        model.fit(X_train_oversampled, y_train_oversampled )  \n",
    "        y_pred = model.predict_proba(X_test)\n",
    "        \n",
    "        print(f'Accuracy: {model.score(X_test, y_test)}')\n",
    "#         print(f'f-score: {f1_score(y_test, y_pred)}')\n",
    "        print(\"AUC : \", roc_auc_score(y_test,y_pred[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modelling without Backward elimination\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = new_train_df.drop(columns=['bidder_id', 'payment_account', 'address', 'outcome','merchandise'])\n",
    "y = new_train_df['outcome']\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-964b142eb7d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCatBoostClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnum_splits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtraining_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_splits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'training_model' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from catboost import CatBoostClassifier\n",
    "model = CatBoostClassifier(learning_rate=0.1, max_depth=3,random_state = 10)\n",
    "num_splits = 10\n",
    "training_model(model, num_splits, X_scaled, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modelling with backward elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = new_train_df[['device', 'time', 'ip', 'num_bids', 'num_first_bids', 'num_last_bids', 'inst_resp', 'perc_inst_resp', 'auto parts', 'books and music', 'clothing', 'computers', 'furniture', 'home goods', 'jewelry', 'mobile', 'office equipment', 'sporting goods', 'num_bids_per_device', 'num_bids_per_country', 'num_bids_per_ip']]\n",
    "y=new_train_df['outcome']\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "def training_model(model, num_splits, X, y):\n",
    "    skfolds = StratifiedKFold(n_splits=num_splits)\n",
    "\n",
    "    for train_index, test_index in skfolds.split(X, y):\n",
    "#         X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "#         y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]        \n",
    "    \n",
    "        sm = SMOTE()\n",
    "        X_train_oversampled, y_train_oversampled = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "        \n",
    "        model = model\n",
    "        model.fit(X_train_oversampled, y_train_oversampled )  \n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        print(f'Accuracy: {model.score(X_test, y_test)}')\n",
    "        print(f'f-score: {f1_score(y_test, y_pred)}')\n",
    "        print(\"AUC : \", roc_auc_score(y_test,y_pred[:,1]))        \n",
    "        \n",
    "model = CatBoostClassifier(learning_rate=0.1, max_depth=3,random_state = 10)\n",
    "num_splits = 10\n",
    "training_model(model, num_splits, X_scaled, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyperparameters tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_scaled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-7e5d24c63f82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_train_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'outcome'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mX_train_oversampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_oversampled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_scaled' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "from scipy.stats import randint\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "X = new_train_df.drop(columns=['bidder_id', 'payment_account', 'address', 'outcome','merchandise'])\n",
    "y = new_train_df['outcome']\n",
    "sm = SMOTE()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.7, random_state=42)\n",
    "X_train_oversampled, y_train_oversampled = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "#Instantiate CatBoostClassifier\n",
    "cbc = CatBoostClassifier()\n",
    "param_dist = { \"learning_rate\": np.linspace(0,0.2,5),\"max_depth\": randint(3, 10)}\n",
    "cbc_random = RandomizedSearchCV(estimator = cbc, param_distributions = param_dist, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "cbc_random.fit(X_train_oversampled,y_train_oversampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_features, test_labels):\n",
    "    predictions = model.predict_proba(test_features)\n",
    "    pred = model.predict(test_features)\n",
    "    \n",
    "    accuracy = accuracy_score(test_labels, pred)\n",
    "    auc_roc_score = roc_auc_score(test_labels,predictions[:,1])\n",
    "    \n",
    "    print('Model Performance')\n",
    "    print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
    "    print('AUC ROC = {:0.2f}%.'.format(auc_roc_score))\n",
    "    \n",
    "    return accuracy, auc_roc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = CatBoostClassifier()\n",
    "base_model.fit(X_train_oversampled, y_train_oversampled)\n",
    "base_accuracy = evaluate(base_model, X_test, y_test)\n",
    "best_random = cbc_random.best_estimator_\n",
    "random_accuracy = evaluate(best_random, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-1ddae9f504f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'classifier' is not defined"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "def create_model():\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(6,activation = 'relu'))\n",
    "    classifier.add(Dense(6, activation = 'relu'))\n",
    "    classifier.add(Dense(1, activation = 'sigmoid'))\n",
    "    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    \n",
    "    \n",
    "classifier.fit(X_train, y_train, batch_size = 10, epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch with 4 hyperparam - weiquan run!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
      "0.879586 (0.026780) with: {'batch_size': 10, 'epochs': 100}\n",
   "outputs": [],
   "source": [
    "\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(learn_rate=0.01, momentum=0):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(6, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load dataset\n",
    "X = new_train_df.drop(columns=['bidder_id', 'payment_account', 'address', 'outcome','merchandise'])\n",
    "y = new_train_df['outcome']\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "sm = SMOTE()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "X_train_oversampled, y_train_oversampled = sm.fit_resample(X_train, y_train)\n",
    "# split into input (X) and output (Y) variables\n",
    "X = X_train_oversampled\n",
    "Y = y_train_oversampled\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "# define the grid search parameters\n",
    "learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "batch_size = [10, 20, 40, 60, 80, 100]\n",
    "epochs = [10, 50, 100]\n",
    "param_grid = dict(learn_rate=learn_rate,momentum=momentum,batch_size=batch_size, epochs=epochs)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X, Y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search for Learn rate and momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.782452 using {'learn_rate': 0.01, 'momentum': 0.6}\n",
      "0.634288 (0.022368) with: {'learn_rate': 0.001, 'momentum': 0.0}\n",
      "0.451458 (0.295540) with: {'learn_rate': 0.001, 'momentum': 0.2}\n",
      "0.706661 (0.054742) with: {'learn_rate': 0.001, 'momentum': 0.4}\n",
      "0.589930 (0.182340) with: {'learn_rate': 0.001, 'momentum': 0.6}\n",
      "0.657129 (0.083882) with: {'learn_rate': 0.001, 'momentum': 0.8}\n",
      "0.515724 (0.166210) with: {'learn_rate': 0.001, 'momentum': 0.9}\n",
      "0.501472 (0.128784) with: {'learn_rate': 0.01, 'momentum': 0.0}\n",
      "0.535504 (0.244883) with: {'learn_rate': 0.01, 'momentum': 0.2}\n",
      "0.565588 (0.150534) with: {'learn_rate': 0.01, 'momentum': 0.4}\n",
      "0.782452 (0.012026) with: {'learn_rate': 0.01, 'momentum': 0.6}\n",
      "0.558078 (0.123363) with: {'learn_rate': 0.01, 'momentum': 0.8}\n",
      "0.514238 (0.116878) with: {'learn_rate': 0.01, 'momentum': 0.9}\n",
      "0.652968 (0.144539) with: {'learn_rate': 0.1, 'momentum': 0.0}\n",
      "0.599025 (0.115265) with: {'learn_rate': 0.1, 'momentum': 0.2}\n",
      "0.595707 (0.193572) with: {'learn_rate': 0.1, 'momentum': 0.4}\n",
      "0.725129 (0.145275) with: {'learn_rate': 0.1, 'momentum': 0.6}\n",
      "0.734086 (0.061088) with: {'learn_rate': 0.1, 'momentum': 0.8}\n",
      "0.482978 (0.271857) with: {'learn_rate': 0.1, 'momentum': 0.9}\n",
      "0.412073 (0.293518) with: {'learn_rate': 0.2, 'momentum': 0.0}\n",
      "0.639206 (0.153340) with: {'learn_rate': 0.2, 'momentum': 0.2}\n",
      "0.676329 (0.066555) with: {'learn_rate': 0.2, 'momentum': 0.4}\n",
      "0.459041 (0.204661) with: {'learn_rate': 0.2, 'momentum': 0.6}\n",
      "0.530285 (0.186491) with: {'learn_rate': 0.2, 'momentum': 0.8}\n",
      "0.596769 (0.066352) with: {'learn_rate': 0.2, 'momentum': 0.9}\n",
      "0.535610 (0.105967) with: {'learn_rate': 0.3, 'momentum': 0.0}\n",
      "0.606106 (0.085391) with: {'learn_rate': 0.3, 'momentum': 0.2}\n",
      "0.724683 (0.002810) with: {'learn_rate': 0.3, 'momentum': 0.4}\n",
      "0.437367 (0.115303) with: {'learn_rate': 0.3, 'momentum': 0.6}\n",
      "0.469539 (0.187870) with: {'learn_rate': 0.3, 'momentum': 0.8}\n",
      "0.773095 (0.067681) with: {'learn_rate': 0.3, 'momentum': 0.9}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(learn_rate=0.01, momentum=0):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(6, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load dataset\n",
    "X = new_train_df.drop(columns=['bidder_id', 'payment_account', 'address', 'outcome','merchandise'])\n",
    "y = new_train_df['outcome']\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "sm = SMOTE()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "X_train_oversampled, y_train_oversampled = sm.fit_resample(X_train, y_train)\n",
    "# split into input (X) and output (Y) variables\n",
    "X = X_train_oversampled\n",
    "Y = y_train_oversampled\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "# define the grid search parameters\n",
    "learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "\n",
    "param_grid = dict(learn_rate=learn_rate, momentum=momentum)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X, Y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomSearch with 4 hyperparameters -weiquan run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brennanszeto/Environments/dsvenv/lib/python3.8/site-packages/sklearn/model_selection/_search.py:278: UserWarning: The total space of parameters 18 is smaller than n_iter=100. Running 18 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   30.0s\n",
      "[Parallel(n_jobs=-1)]: Done  54 out of  54 | elapsed:   46.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.907304 using {'epochs': 100, 'batch_size': 10}\n",
      "0.681946 (0.149657) with: {'epochs': 10, 'batch_size': 10}\n",
      "0.779205 (0.102228) with: {'epochs': 50, 'batch_size': 10}\n",
      "0.907304 (0.048757) with: {'epochs': 100, 'batch_size': 10}\n",
      "0.682802 (0.137671) with: {'epochs': 10, 'batch_size': 20}\n",
      "0.713806 (0.126166) with: {'epochs': 50, 'batch_size': 20}\n",
      "0.861342 (0.037683) with: {'epochs': 100, 'batch_size': 20}\n",
      "0.580539 (0.135932) with: {'epochs': 10, 'batch_size': 40}\n",
      "0.697875 (0.104039) with: {'epochs': 50, 'batch_size': 40}\n",
      "0.756200 (0.103668) with: {'epochs': 100, 'batch_size': 40}\n",
      "0.559254 (0.146420) with: {'epochs': 10, 'batch_size': 60}\n",
      "0.708457 (0.109958) with: {'epochs': 50, 'batch_size': 60}\n",
      "0.782716 (0.076441) with: {'epochs': 100, 'batch_size': 60}\n",
      "0.469243 (0.215609) with: {'epochs': 10, 'batch_size': 80}\n",
      "0.691688 (0.140931) with: {'epochs': 50, 'batch_size': 80}\n",
      "0.724325 (0.139656) with: {'epochs': 100, 'batch_size': 80}\n",
      "0.620905 (0.164163) with: {'epochs': 10, 'batch_size': 100}\n",
      "0.649297 (0.137553) with: {'epochs': 50, 'batch_size': 100}\n",
      "0.691641 (0.170397) with: {'epochs': 100, 'batch_size': 100}\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(learn_rate=0.01, momentum=0):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(6, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load dataset\n",
    "X = new_train_df.drop(columns=['bidder_id', 'payment_account', 'address', 'outcome','merchandise'])\n",
    "y = new_train_df['outcome']\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "sm = SMOTE()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.7, random_state=42)\n",
    "X_train_oversampled, y_train_oversampled = sm.fit_resample(X_train, y_train)\n",
    "# split into input (X) and output (Y) variables\n",
    "X = X_train_oversampled\n",
    "Y = y_train_oversampled\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "# define the grid search parameters\n",
    "learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "batch_size = [10, 20, 40, 60, 80, 100]\n",
    "epochs = [10, 50, 100]\n",
    "param_grid = dict(learn_rate=learn_rate,momentum=momentum,batch_size=batch_size, epochs=epochs)\n",
    "# random = RandomizedSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "random = RandomizedSearchCV(estimator = model, param_distributions = param_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "random_result = random.fit(X, Y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (random_result.best_score_, random_result.best_params_))\n",
    "means = random_result.cv_results_['mean_test_score']\n",
    "stds = random_result.cv_results_['std_test_score']\n",
    "params = random_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Tuner Random Search\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project ./untitled_project/oracle.json\n",
      "INFO:tensorflow:Reloading Tuner from ./untitled_project/tuner0.json\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "\n",
      "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
      "layer is 64 and the optimal learning rate for the optimizer\n",
      "is 0.01.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras_tuner as kt\n",
    "\n",
    "\n",
    "def model_builder(hp):\n",
    "  model = keras.Sequential()\n",
    "  model.add(keras.layers.Flatten())\n",
    "\n",
    "  # Tune the number of units in the first Dense layer\n",
    "  # Choose an optimal value between 32-512\n",
    "  hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
    "  model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n",
    "  model.add(keras.layers.Dense(10))\n",
    "\n",
    "  # Tune the learning rate for the optimizer\n",
    "  # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
    "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "  return model\n",
    "\n",
    "X = new_train_df.drop(columns=['bidder_id', 'payment_account', 'address', 'outcome','merchandise'])\n",
    "y = new_train_df['outcome']\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "sm = SMOTE()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "X_train_oversampled, y_train_oversampled = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "tuner = kt.RandomSearch(model_builder,\n",
    "                     objective='val_accuracy',\n",
    "                    max_trials = 4\n",
    "                       )\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "tuner.search(X_train_oversampled, y_train_oversampled, epochs=50, validation_split=0.2, callbacks=[stop_early])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
    "is {best_hps.get('learning_rate')}.\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "67/67 [==============================] - 0s 2ms/step - loss: 0.5829 - accuracy: 0.7795 - val_loss: 0.5310 - val_accuracy: 0.7228\n",
      "Epoch 2/50\n",
      "67/67 [==============================] - 0s 890us/step - loss: 0.3141 - accuracy: 0.8715 - val_loss: 0.4729 - val_accuracy: 0.7809\n",
      "Epoch 3/50\n",
      "67/67 [==============================] - 0s 838us/step - loss: 0.2507 - accuracy: 0.9043 - val_loss: 0.2667 - val_accuracy: 0.9157\n",
      "Epoch 4/50\n",
      "67/67 [==============================] - 0s 847us/step - loss: 0.2027 - accuracy: 0.9278 - val_loss: 0.2514 - val_accuracy: 0.9007\n",
      "Epoch 5/50\n",
      "67/67 [==============================] - 0s 827us/step - loss: 0.1714 - accuracy: 0.9371 - val_loss: 0.1284 - val_accuracy: 0.9588\n",
      "Epoch 6/50\n",
      "67/67 [==============================] - 0s 854us/step - loss: 0.1376 - accuracy: 0.9508 - val_loss: 0.1567 - val_accuracy: 0.9682\n",
      "Epoch 7/50\n",
      "67/67 [==============================] - 0s 847us/step - loss: 0.1485 - accuracy: 0.9550 - val_loss: 0.1137 - val_accuracy: 0.9569\n",
      "Epoch 8/50\n",
      "67/67 [==============================] - 0s 896us/step - loss: 0.1146 - accuracy: 0.9601 - val_loss: 0.1263 - val_accuracy: 0.9513\n",
      "Epoch 9/50\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.0980 - accuracy: 0.9644 - val_loss: 0.0697 - val_accuracy: 0.9794\n",
      "Epoch 10/50\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.0958 - accuracy: 0.9629 - val_loss: 0.1537 - val_accuracy: 0.9457\n",
      "Epoch 11/50\n",
      "67/67 [==============================] - 0s 969us/step - loss: 0.0858 - accuracy: 0.9681 - val_loss: 0.1627 - val_accuracy: 0.9625\n",
      "Epoch 12/50\n",
      "67/67 [==============================] - 0s 843us/step - loss: 0.0932 - accuracy: 0.9676 - val_loss: 0.1407 - val_accuracy: 0.9345\n",
      "Epoch 13/50\n",
      "67/67 [==============================] - 0s 830us/step - loss: 0.0815 - accuracy: 0.9737 - val_loss: 0.1653 - val_accuracy: 0.9419\n",
      "Epoch 14/50\n",
      "67/67 [==============================] - 0s 895us/step - loss: 0.0690 - accuracy: 0.9709 - val_loss: 0.0884 - val_accuracy: 0.9682\n",
      "Epoch 15/50\n",
      "67/67 [==============================] - 0s 850us/step - loss: 0.0646 - accuracy: 0.9747 - val_loss: 0.0263 - val_accuracy: 0.9944\n",
      "Epoch 16/50\n",
      "67/67 [==============================] - 0s 851us/step - loss: 0.0645 - accuracy: 0.9714 - val_loss: 0.0495 - val_accuracy: 0.9906\n",
      "Epoch 17/50\n",
      "67/67 [==============================] - 0s 853us/step - loss: 0.0521 - accuracy: 0.9780 - val_loss: 0.0855 - val_accuracy: 0.9738\n",
      "Epoch 18/50\n",
      "67/67 [==============================] - 0s 915us/step - loss: 0.0510 - accuracy: 0.9803 - val_loss: 0.0277 - val_accuracy: 0.9944\n",
      "Epoch 19/50\n",
      "67/67 [==============================] - 0s 822us/step - loss: 0.0437 - accuracy: 0.9808 - val_loss: 0.0398 - val_accuracy: 0.9850\n",
      "Epoch 20/50\n",
      "67/67 [==============================] - 0s 824us/step - loss: 0.0497 - accuracy: 0.9784 - val_loss: 0.0395 - val_accuracy: 0.9944\n",
      "Epoch 21/50\n",
      "67/67 [==============================] - 0s 830us/step - loss: 0.0492 - accuracy: 0.9789 - val_loss: 0.1076 - val_accuracy: 0.9551\n",
      "Epoch 22/50\n",
      "67/67 [==============================] - 0s 859us/step - loss: 0.0471 - accuracy: 0.9817 - val_loss: 0.0387 - val_accuracy: 0.9869\n",
      "Epoch 23/50\n",
      "67/67 [==============================] - 0s 808us/step - loss: 0.0395 - accuracy: 0.9841 - val_loss: 0.0456 - val_accuracy: 0.9794\n",
      "Epoch 24/50\n",
      "67/67 [==============================] - 0s 832us/step - loss: 0.0374 - accuracy: 0.9845 - val_loss: 0.1196 - val_accuracy: 0.9494\n",
      "Epoch 25/50\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.0595 - accuracy: 0.9761 - val_loss: 0.0584 - val_accuracy: 0.9794\n",
      "Epoch 26/50\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.0471 - accuracy: 0.9803 - val_loss: 0.0250 - val_accuracy: 0.9925\n",
      "Epoch 27/50\n",
      "67/67 [==============================] - 0s 846us/step - loss: 0.0406 - accuracy: 0.9822 - val_loss: 0.0225 - val_accuracy: 0.9944\n",
      "Epoch 28/50\n",
      "67/67 [==============================] - 0s 850us/step - loss: 0.0416 - accuracy: 0.9836 - val_loss: 0.1639 - val_accuracy: 0.9682\n",
      "Epoch 29/50\n",
      "67/67 [==============================] - 0s 849us/step - loss: 0.1168 - accuracy: 0.9667 - val_loss: 0.0432 - val_accuracy: 0.9869\n",
      "Epoch 30/50\n",
      "67/67 [==============================] - 0s 865us/step - loss: 0.0849 - accuracy: 0.9723 - val_loss: 0.1726 - val_accuracy: 0.9757\n",
      "Epoch 31/50\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.0951 - accuracy: 0.9723 - val_loss: 0.1585 - val_accuracy: 0.9363\n",
      "Epoch 32/50\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.0793 - accuracy: 0.9798 - val_loss: 0.1136 - val_accuracy: 0.9831\n",
      "Epoch 33/50\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.0515 - accuracy: 0.9794 - val_loss: 0.0919 - val_accuracy: 0.9719\n",
      "Epoch 34/50\n",
      "67/67 [==============================] - 0s 832us/step - loss: 0.0373 - accuracy: 0.9869 - val_loss: 0.0308 - val_accuracy: 0.9963\n",
      "Epoch 35/50\n",
      "67/67 [==============================] - 0s 851us/step - loss: 0.0369 - accuracy: 0.9831 - val_loss: 0.0329 - val_accuracy: 0.9850\n",
      "Epoch 36/50\n",
      "67/67 [==============================] - 0s 855us/step - loss: 0.0363 - accuracy: 0.9845 - val_loss: 0.0419 - val_accuracy: 0.9850\n",
      "Epoch 37/50\n",
      "67/67 [==============================] - 0s 834us/step - loss: 0.0313 - accuracy: 0.9887 - val_loss: 0.0167 - val_accuracy: 0.9944\n",
      "Epoch 38/50\n",
      "67/67 [==============================] - 0s 822us/step - loss: 0.0400 - accuracy: 0.9850 - val_loss: 0.0607 - val_accuracy: 0.9794\n",
      "Epoch 39/50\n",
      "67/67 [==============================] - 0s 824us/step - loss: 0.0276 - accuracy: 0.9873 - val_loss: 0.0255 - val_accuracy: 0.9888\n",
      "Epoch 40/50\n",
      "67/67 [==============================] - 0s 830us/step - loss: 0.0266 - accuracy: 0.9878 - val_loss: 0.0118 - val_accuracy: 0.9963\n",
      "Epoch 41/50\n",
      "67/67 [==============================] - 0s 829us/step - loss: 0.0246 - accuracy: 0.9873 - val_loss: 0.0584 - val_accuracy: 0.9700\n",
      "Epoch 42/50\n",
      "67/67 [==============================] - 0s 808us/step - loss: 0.0282 - accuracy: 0.9859 - val_loss: 0.0436 - val_accuracy: 0.9794\n",
      "Epoch 43/50\n",
      "67/67 [==============================] - 0s 838us/step - loss: 0.0268 - accuracy: 0.9887 - val_loss: 0.0312 - val_accuracy: 0.9850\n",
      "Epoch 44/50\n",
      "67/67 [==============================] - 0s 805us/step - loss: 0.0380 - accuracy: 0.9841 - val_loss: 0.0392 - val_accuracy: 0.9813\n",
      "Epoch 45/50\n",
      "67/67 [==============================] - 0s 834us/step - loss: 0.0343 - accuracy: 0.9869 - val_loss: 0.0591 - val_accuracy: 0.9738\n",
      "Epoch 46/50\n",
      "67/67 [==============================] - 0s 816us/step - loss: 0.0344 - accuracy: 0.9850 - val_loss: 0.0720 - val_accuracy: 0.9719\n",
      "Epoch 47/50\n",
      "67/67 [==============================] - 0s 834us/step - loss: 0.0321 - accuracy: 0.9855 - val_loss: 0.0248 - val_accuracy: 0.9925\n",
      "Epoch 48/50\n",
      "67/67 [==============================] - 0s 817us/step - loss: 0.0252 - accuracy: 0.9883 - val_loss: 0.0274 - val_accuracy: 0.9925\n",
      "Epoch 49/50\n",
      "67/67 [==============================] - 0s 822us/step - loss: 0.0271 - accuracy: 0.9850 - val_loss: 0.0521 - val_accuracy: 0.9719\n",
      "Epoch 50/50\n",
      "67/67 [==============================] - 0s 821us/step - loss: 0.0222 - accuracy: 0.9887 - val_loss: 0.0123 - val_accuracy: 0.9944\n",
      "Best epoch: 34\n"
     ]
    }
   ],
   "source": [
    "# Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(X_train_oversampled, y_train_oversampled, epochs=50, validation_split=0.2)\n",
    "\n",
    "val_acc_per_epoch = history.history['val_accuracy']\n",
    "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "print('Best epoch: %d' % (best_epoch,))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.921437 using {'learn_rate': 0.3, 'momentum': 0.4}\n",
      "0.888763 (0.059324) with: {'learn_rate': 0.001, 'momentum': 0.0}\n",
      "0.899335 (0.043818) with: {'learn_rate': 0.001, 'momentum': 0.2}\n",
      "0.895822 (0.049798) with: {'learn_rate': 0.001, 'momentum': 0.4}\n",
      "0.905517 (0.034849) with: {'learn_rate': 0.001, 'momentum': 0.6}\n",
      "0.871095 (0.057871) with: {'learn_rate': 0.001, 'momentum': 0.8}\n",
      "0.906430 (0.055439) with: {'learn_rate': 0.001, 'momentum': 0.9}\n",
      "0.806581 (0.055063) with: {'learn_rate': 0.01, 'momentum': 0.0}\n",
      "0.869283 (0.053607) with: {'learn_rate': 0.01, 'momentum': 0.2}\n",
      "0.884363 (0.070057) with: {'learn_rate': 0.01, 'momentum': 0.4}\n",
      "0.895810 (0.042154) with: {'learn_rate': 0.01, 'momentum': 0.6}\n",
      "0.856930 (0.047736) with: {'learn_rate': 0.01, 'momentum': 0.8}\n",
      "0.895820 (0.053047) with: {'learn_rate': 0.01, 'momentum': 0.9}\n",
      "0.810958 (0.100750) with: {'learn_rate': 0.1, 'momentum': 0.0}\n",
      "0.907288 (0.042124) with: {'learn_rate': 0.1, 'momentum': 0.2}\n",
      "0.895822 (0.049798) with: {'learn_rate': 0.1, 'momentum': 0.4}\n",
      "0.898495 (0.068832) with: {'learn_rate': 0.1, 'momentum': 0.6}\n",
      "0.888751 (0.048746) with: {'learn_rate': 0.1, 'momentum': 0.8}\n",
      "0.862184 (0.061995) with: {'learn_rate': 0.1, 'momentum': 0.9}\n",
      "0.898481 (0.057454) with: {'learn_rate': 0.2, 'momentum': 0.0}\n",
      "0.890522 (0.050539) with: {'learn_rate': 0.2, 'momentum': 0.2}\n",
      "0.887871 (0.052045) with: {'learn_rate': 0.2, 'momentum': 0.4}\n",
      "0.891392 (0.040377) with: {'learn_rate': 0.2, 'momentum': 0.6}\n",
      "0.850757 (0.055381) with: {'learn_rate': 0.2, 'momentum': 0.8}\n",
      "0.868450 (0.060616) with: {'learn_rate': 0.2, 'momentum': 0.9}\n",
      "0.835724 (0.073699) with: {'learn_rate': 0.3, 'momentum': 0.0}\n",
      "0.894973 (0.077379) with: {'learn_rate': 0.3, 'momentum': 0.2}\n",
      "0.921437 (0.047777) with: {'learn_rate': 0.3, 'momentum': 0.4}\n",
      "0.866640 (0.025992) with: {'learn_rate': 0.3, 'momentum': 0.6}\n",
      "0.878124 (0.045108) with: {'learn_rate': 0.3, 'momentum': 0.8}\n",
      "0.879890 (0.038755) with: {'learn_rate': 0.3, 'momentum': 0.9}\n"
     ]
    }
   ],
   "source": [
    "# Use scikit-learn to grid search the learning rate and momentum\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# from tf.keras.optimizers import SGD\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(learn_rate=0.01, momentum=0):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(6, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "X = X_train_oversampled\n",
    "Y = y_train_oversampled\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)\n",
    "# define the grid search parameters\n",
    "learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "param_grid = dict(learn_rate=learn_rate, momentum=momentum)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X, Y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with autoML libraries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TPOT pipeline creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier\n",
    "pipeline_optimizer = TPOTClassifier()\n",
    "pipeline_optimizer = TPOTClassifier(generations=5, population_size=20, cv=5,\n",
    "                                    random_state=42, verbosity=2)\n",
    "pipeline_optimizer.fit(X_train, y_train)\n",
    "print(pipeline_optimizer.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyCaret classifier creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.datasets import get_data\n",
    "data = new_train_df\n",
    "from pycaret.classification import *\n",
    "exp_name = setup(data = new_train_df,target = 'outcome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = compare_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pycaret.datasets import get_data\n",
    "\n",
    "\n",
    "from pycaret.classification import *\n",
    "\n",
    "exp_name = setup(data = data,  target = 'outcome',silent=True)\n",
    "\n",
    "cb = create_model('catboost')\n",
    "\n",
    "optimize_threshold(cb, true_negative = 10, false_negative = -100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
