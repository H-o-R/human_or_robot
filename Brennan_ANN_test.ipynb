{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as  pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# import xgboost as xgb\n",
    "# from catboost import CatBoostClassifier      \n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "# from sklearn.model_selection import KFold\n",
    "# import statistics\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature engineered dfs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_df = pd.read_csv('new_train.csv')\n",
    "new_test_df = pd.read_csv('new_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# sns.countplot(new_train_df['outcome'],label=\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_train_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# corr = new_train_df.iloc[:,:].corr()\n",
    "# colormap = sns.diverging_palette(220, 10, as_cmap = True)\n",
    "# plt.figure(figsize=(14,14))\n",
    "# sns.heatmap(corr, cbar = True,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 12},\n",
    "#             cmap = colormap, linewidths=0.1, linecolor='white')\n",
    "# plt.title('Correlation of new_train Features', y=1.05, size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified K-Fold with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def training_model(model, num_splits, X, y):\n",
    "    skfolds = StratifiedKFold(n_splits=num_splits)\n",
    "\n",
    "    for train_index, test_index in skfolds.split(X, y):\n",
    "#         X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "#         y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]        \n",
    "    \n",
    "        sm = SMOTE()\n",
    "        X_train_oversampled, y_train_oversampled = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "        \n",
    "        model = model\n",
    "        model.fit(X_train_oversampled, y_train_oversampled )  \n",
    "        y_pred = model.predict_proba(X_test)\n",
    "        \n",
    "        print(f'Accuracy: {model.score(X_test, y_test)}')\n",
    "#         print(f'f-score: {f1_score(y_test, y_pred)}')\n",
    "        print(\"AUC : \", roc_auc_score(y_test,y_pred[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modelling without Backward elimination\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = new_train_df.drop(columns=['bidder_id', 'payment_account', 'address', 'outcome','merchandise'])\n",
    "y = new_train_df['outcome']\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-964b142eb7d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCatBoostClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnum_splits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtraining_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_splits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'training_model' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from catboost import CatBoostClassifier\n",
    "model = CatBoostClassifier(learning_rate=0.1, max_depth=3,random_state = 10)\n",
    "num_splits = 10\n",
    "training_model(model, num_splits, X_scaled, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modelling with backward elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = new_train_df[['device', 'time', 'ip', 'num_bids', 'num_first_bids', 'num_last_bids', 'inst_resp', 'perc_inst_resp', 'auto parts', 'books and music', 'clothing', 'computers', 'furniture', 'home goods', 'jewelry', 'mobile', 'office equipment', 'sporting goods', 'num_bids_per_device', 'num_bids_per_country', 'num_bids_per_ip']]\n",
    "y=new_train_df['outcome']\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "def training_model(model, num_splits, X, y):\n",
    "    skfolds = StratifiedKFold(n_splits=num_splits)\n",
    "\n",
    "    for train_index, test_index in skfolds.split(X, y):\n",
    "#         X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "#         y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]        \n",
    "    \n",
    "        sm = SMOTE()\n",
    "        X_train_oversampled, y_train_oversampled = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "        \n",
    "        model = model\n",
    "        model.fit(X_train_oversampled, y_train_oversampled )  \n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        print(f'Accuracy: {model.score(X_test, y_test)}')\n",
    "        print(f'f-score: {f1_score(y_test, y_pred)}')\n",
    "        print(\"AUC : \", roc_auc_score(y_test,y_pred[:,1]))        \n",
    "        \n",
    "model = CatBoostClassifier(learning_rate=0.1, max_depth=3,random_state = 10)\n",
    "num_splits = 10\n",
    "training_model(model, num_splits, X_scaled, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyperparameters tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_scaled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-7e5d24c63f82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_train_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'outcome'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mX_train_oversampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_oversampled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_scaled' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "from scipy.stats import randint\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "X = new_train_df.drop(columns=['bidder_id', 'payment_account', 'address', 'outcome','merchandise'])\n",
    "y = new_train_df['outcome']\n",
    "sm = SMOTE()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.7, random_state=42)\n",
    "X_train_oversampled, y_train_oversampled = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "#Instantiate CatBoostClassifier\n",
    "cbc = CatBoostClassifier()\n",
    "param_dist = { \"learning_rate\": np.linspace(0,0.2,5),\"max_depth\": randint(3, 10)}\n",
    "cbc_random = RandomizedSearchCV(estimator = cbc, param_distributions = param_dist, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "cbc_random.fit(X_train_oversampled,y_train_oversampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_features, test_labels):\n",
    "    predictions = model.predict_proba(test_features)\n",
    "    pred = model.predict(test_features)\n",
    "    \n",
    "    accuracy = accuracy_score(test_labels, pred)\n",
    "    auc_roc_score = roc_auc_score(test_labels,predictions[:,1])\n",
    "    \n",
    "    print('Model Performance')\n",
    "    print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
    "    print('AUC ROC = {:0.2f}%.'.format(auc_roc_score))\n",
    "    \n",
    "    return accuracy, auc_roc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = CatBoostClassifier()\n",
    "base_model.fit(X_train_oversampled, y_train_oversampled)\n",
    "base_accuracy = evaluate(base_model, X_test, y_test)\n",
    "best_random = cbc_random.best_estimator_\n",
    "random_accuracy = evaluate(best_random, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-1ddae9f504f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'classifier' is not defined"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "def create_model():\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(6,activation = 'relu'))\n",
    "    classifier.add(Dense(6, activation = 'relu'))\n",
    "    classifier.add(Dense(1, activation = 'sigmoid'))\n",
    "    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    \n",
    "    \n",
    "classifier.fit(X_train, y_train, batch_size = 10, epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.879586 using {'batch_size': 10, 'epochs': 100}\n",
      "0.690860 (0.128256) with: {'batch_size': 10, 'epochs': 10}\n",
      "0.833085 (0.054751) with: {'batch_size': 10, 'epochs': 50}\n",
      "0.879586 (0.026780) with: {'batch_size': 10, 'epochs': 100}\n",
      "0.717145 (0.090323) with: {'batch_size': 20, 'epochs': 10}\n",
      "0.804549 (0.058911) with: {'batch_size': 20, 'epochs': 50}\n",
      "0.864593 (0.049771) with: {'batch_size': 20, 'epochs': 100}\n",
      "0.733277 (0.073329) with: {'batch_size': 40, 'epochs': 10}\n",
      "0.752780 (0.088670) with: {'batch_size': 40, 'epochs': 50}\n",
      "0.862711 (0.052326) with: {'batch_size': 40, 'epochs': 100}\n",
      "0.699493 (0.112963) with: {'batch_size': 60, 'epochs': 10}\n",
      "0.768916 (0.070851) with: {'batch_size': 60, 'epochs': 50}\n",
      "0.809445 (0.069267) with: {'batch_size': 60, 'epochs': 100}\n",
      "0.656397 (0.110732) with: {'batch_size': 80, 'epochs': 10}\n",
      "0.762169 (0.064864) with: {'batch_size': 80, 'epochs': 50}\n",
      "0.798543 (0.071923) with: {'batch_size': 80, 'epochs': 100}\n",
      "0.703647 (0.066646) with: {'batch_size': 100, 'epochs': 10}\n",
      "0.737404 (0.083465) with: {'batch_size': 100, 'epochs': 50}\n",
      "0.753160 (0.083605) with: {'batch_size': 100, 'epochs': 100}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(6, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load dataset\n",
    "X = new_train_df.drop(columns=['bidder_id', 'payment_account', 'address', 'outcome','merchandise'])\n",
    "y = new_train_df['outcome']\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "sm = SMOTE()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "X_train_oversampled, y_train_oversampled = sm.fit_resample(X_train, y_train)\n",
    "# split into input (X) and output (Y) variables\n",
    "X = X_train_oversampled\n",
    "Y = y_train_oversampled\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "# define the grid search parameters\n",
    "# learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "# momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "batch_size = [10, 20, 40, 60, 80, 100]\n",
    "epochs = [10, 50, 100]\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X, Y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "learn_rate is not a legal parameter",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Users/brennanszeto/Environments/dsvenv/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py\", line 431, in _process_worker\n    r = call_item()\n  File \"/Users/brennanszeto/Environments/dsvenv/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py\", line 285, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/Users/brennanszeto/Environments/dsvenv/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 595, in __call__\n    return self.func(*args, **kwargs)\n  File \"/Users/brennanszeto/Environments/dsvenv/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in __call__\n    return [func(*args, **kwargs)\n  File \"/Users/brennanszeto/Environments/dsvenv/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"/Users/brennanszeto/Environments/dsvenv/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 520, in _fit_and_score\n    estimator = estimator.set_params(**cloned_parameters)\n  File \"/Users/brennanszeto/Environments/dsvenv/lib/python3.8/site-packages/tensorflow/python/keras/wrappers/scikit_learn.py\", line 127, in set_params\n    self.check_params(params)\n  File \"/Users/brennanszeto/Environments/dsvenv/lib/python3.8/site-packages/tensorflow/python/keras/wrappers/scikit_learn.py\", line 103, in check_params\n    raise ValueError('{} is not a legal parameter'.format(params_name))\nValueError: learn_rate is not a legal parameter\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-3e12e3182169>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearn_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mgrid_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;31m# summarize results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best: %f using %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgrid_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/dsvenv/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/dsvenv/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/dsvenv/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1188\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/dsvenv/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    706\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[1;32m    709\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/dsvenv/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/dsvenv/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/dsvenv/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: learn_rate is not a legal parameter"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(6, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load dataset\n",
    "X = new_train_df.drop(columns=['bidder_id', 'payment_account', 'address', 'outcome','merchandise'])\n",
    "y = new_train_df['outcome']\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "sm = SMOTE()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "X_train_oversampled, y_train_oversampled = sm.fit_resample(X_train, y_train)\n",
    "# split into input (X) and output (Y) variables\n",
    "X = X_train_oversampled\n",
    "Y = y_train_oversampled\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "# define the grid search parameters\n",
    "learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "\n",
    "param_grid = dict(learn_rate=learn_rate, momentum=momentum)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X, Y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Tuner\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project ./untitled_project/oracle.json\n",
      "INFO:tensorflow:Reloading Tuner from ./untitled_project/tuner0.json\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "\n",
      "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
      "layer is 64 and the optimal learning rate for the optimizer\n",
      "is 0.01.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras_tuner as kt\n",
    "\n",
    "\n",
    "def model_builder(hp):\n",
    "  model = keras.Sequential()\n",
    "  model.add(keras.layers.Flatten())\n",
    "\n",
    "  # Tune the number of units in the first Dense layer\n",
    "  # Choose an optimal value between 32-512\n",
    "  hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
    "  model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n",
    "  model.add(keras.layers.Dense(10))\n",
    "\n",
    "  # Tune the learning rate for the optimizer\n",
    "  # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
    "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "  return model\n",
    "\n",
    "X = new_train_df.drop(columns=['bidder_id', 'payment_account', 'address', 'outcome','merchandise'])\n",
    "y = new_train_df['outcome']\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "sm = SMOTE()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "X_train_oversampled, y_train_oversampled = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "tuner = kt.RandomSearch(model_builder,\n",
    "                     objective='val_accuracy',\n",
    "                    max_trials = 4\n",
    "                       )\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "tuner.search(X_train_oversampled, y_train_oversampled, epochs=50, validation_split=0.2, callbacks=[stop_early])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
    "is {best_hps.get('learning_rate')}.\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "67/67 [==============================] - 0s 2ms/step - loss: 0.5829 - accuracy: 0.7795 - val_loss: 0.5310 - val_accuracy: 0.7228\n",
      "Epoch 2/50\n",
      "67/67 [==============================] - 0s 890us/step - loss: 0.3141 - accuracy: 0.8715 - val_loss: 0.4729 - val_accuracy: 0.7809\n",
      "Epoch 3/50\n",
      "67/67 [==============================] - 0s 838us/step - loss: 0.2507 - accuracy: 0.9043 - val_loss: 0.2667 - val_accuracy: 0.9157\n",
      "Epoch 4/50\n",
      "67/67 [==============================] - 0s 847us/step - loss: 0.2027 - accuracy: 0.9278 - val_loss: 0.2514 - val_accuracy: 0.9007\n",
      "Epoch 5/50\n",
      "67/67 [==============================] - 0s 827us/step - loss: 0.1714 - accuracy: 0.9371 - val_loss: 0.1284 - val_accuracy: 0.9588\n",
      "Epoch 6/50\n",
      "67/67 [==============================] - 0s 854us/step - loss: 0.1376 - accuracy: 0.9508 - val_loss: 0.1567 - val_accuracy: 0.9682\n",
      "Epoch 7/50\n",
      "67/67 [==============================] - 0s 847us/step - loss: 0.1485 - accuracy: 0.9550 - val_loss: 0.1137 - val_accuracy: 0.9569\n",
      "Epoch 8/50\n",
      "67/67 [==============================] - 0s 896us/step - loss: 0.1146 - accuracy: 0.9601 - val_loss: 0.1263 - val_accuracy: 0.9513\n",
      "Epoch 9/50\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.0980 - accuracy: 0.9644 - val_loss: 0.0697 - val_accuracy: 0.9794\n",
      "Epoch 10/50\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.0958 - accuracy: 0.9629 - val_loss: 0.1537 - val_accuracy: 0.9457\n",
      "Epoch 11/50\n",
      "67/67 [==============================] - 0s 969us/step - loss: 0.0858 - accuracy: 0.9681 - val_loss: 0.1627 - val_accuracy: 0.9625\n",
      "Epoch 12/50\n",
      "67/67 [==============================] - 0s 843us/step - loss: 0.0932 - accuracy: 0.9676 - val_loss: 0.1407 - val_accuracy: 0.9345\n",
      "Epoch 13/50\n",
      "67/67 [==============================] - 0s 830us/step - loss: 0.0815 - accuracy: 0.9737 - val_loss: 0.1653 - val_accuracy: 0.9419\n",
      "Epoch 14/50\n",
      "67/67 [==============================] - 0s 895us/step - loss: 0.0690 - accuracy: 0.9709 - val_loss: 0.0884 - val_accuracy: 0.9682\n",
      "Epoch 15/50\n",
      "67/67 [==============================] - 0s 850us/step - loss: 0.0646 - accuracy: 0.9747 - val_loss: 0.0263 - val_accuracy: 0.9944\n",
      "Epoch 16/50\n",
      "67/67 [==============================] - 0s 851us/step - loss: 0.0645 - accuracy: 0.9714 - val_loss: 0.0495 - val_accuracy: 0.9906\n",
      "Epoch 17/50\n",
      "67/67 [==============================] - 0s 853us/step - loss: 0.0521 - accuracy: 0.9780 - val_loss: 0.0855 - val_accuracy: 0.9738\n",
      "Epoch 18/50\n",
      "67/67 [==============================] - 0s 915us/step - loss: 0.0510 - accuracy: 0.9803 - val_loss: 0.0277 - val_accuracy: 0.9944\n",
      "Epoch 19/50\n",
      "67/67 [==============================] - 0s 822us/step - loss: 0.0437 - accuracy: 0.9808 - val_loss: 0.0398 - val_accuracy: 0.9850\n",
      "Epoch 20/50\n",
      "67/67 [==============================] - 0s 824us/step - loss: 0.0497 - accuracy: 0.9784 - val_loss: 0.0395 - val_accuracy: 0.9944\n",
      "Epoch 21/50\n",
      "67/67 [==============================] - 0s 830us/step - loss: 0.0492 - accuracy: 0.9789 - val_loss: 0.1076 - val_accuracy: 0.9551\n",
      "Epoch 22/50\n",
      "67/67 [==============================] - 0s 859us/step - loss: 0.0471 - accuracy: 0.9817 - val_loss: 0.0387 - val_accuracy: 0.9869\n",
      "Epoch 23/50\n",
      "67/67 [==============================] - 0s 808us/step - loss: 0.0395 - accuracy: 0.9841 - val_loss: 0.0456 - val_accuracy: 0.9794\n",
      "Epoch 24/50\n",
      "67/67 [==============================] - 0s 832us/step - loss: 0.0374 - accuracy: 0.9845 - val_loss: 0.1196 - val_accuracy: 0.9494\n",
      "Epoch 25/50\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.0595 - accuracy: 0.9761 - val_loss: 0.0584 - val_accuracy: 0.9794\n",
      "Epoch 26/50\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.0471 - accuracy: 0.9803 - val_loss: 0.0250 - val_accuracy: 0.9925\n",
      "Epoch 27/50\n",
      "67/67 [==============================] - 0s 846us/step - loss: 0.0406 - accuracy: 0.9822 - val_loss: 0.0225 - val_accuracy: 0.9944\n",
      "Epoch 28/50\n",
      "67/67 [==============================] - 0s 850us/step - loss: 0.0416 - accuracy: 0.9836 - val_loss: 0.1639 - val_accuracy: 0.9682\n",
      "Epoch 29/50\n",
      "67/67 [==============================] - 0s 849us/step - loss: 0.1168 - accuracy: 0.9667 - val_loss: 0.0432 - val_accuracy: 0.9869\n",
      "Epoch 30/50\n",
      "67/67 [==============================] - 0s 865us/step - loss: 0.0849 - accuracy: 0.9723 - val_loss: 0.1726 - val_accuracy: 0.9757\n",
      "Epoch 31/50\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.0951 - accuracy: 0.9723 - val_loss: 0.1585 - val_accuracy: 0.9363\n",
      "Epoch 32/50\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.0793 - accuracy: 0.9798 - val_loss: 0.1136 - val_accuracy: 0.9831\n",
      "Epoch 33/50\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.0515 - accuracy: 0.9794 - val_loss: 0.0919 - val_accuracy: 0.9719\n",
      "Epoch 34/50\n",
      "67/67 [==============================] - 0s 832us/step - loss: 0.0373 - accuracy: 0.9869 - val_loss: 0.0308 - val_accuracy: 0.9963\n",
      "Epoch 35/50\n",
      "67/67 [==============================] - 0s 851us/step - loss: 0.0369 - accuracy: 0.9831 - val_loss: 0.0329 - val_accuracy: 0.9850\n",
      "Epoch 36/50\n",
      "67/67 [==============================] - 0s 855us/step - loss: 0.0363 - accuracy: 0.9845 - val_loss: 0.0419 - val_accuracy: 0.9850\n",
      "Epoch 37/50\n",
      "67/67 [==============================] - 0s 834us/step - loss: 0.0313 - accuracy: 0.9887 - val_loss: 0.0167 - val_accuracy: 0.9944\n",
      "Epoch 38/50\n",
      "67/67 [==============================] - 0s 822us/step - loss: 0.0400 - accuracy: 0.9850 - val_loss: 0.0607 - val_accuracy: 0.9794\n",
      "Epoch 39/50\n",
      "67/67 [==============================] - 0s 824us/step - loss: 0.0276 - accuracy: 0.9873 - val_loss: 0.0255 - val_accuracy: 0.9888\n",
      "Epoch 40/50\n",
      "67/67 [==============================] - 0s 830us/step - loss: 0.0266 - accuracy: 0.9878 - val_loss: 0.0118 - val_accuracy: 0.9963\n",
      "Epoch 41/50\n",
      "67/67 [==============================] - 0s 829us/step - loss: 0.0246 - accuracy: 0.9873 - val_loss: 0.0584 - val_accuracy: 0.9700\n",
      "Epoch 42/50\n",
      "67/67 [==============================] - 0s 808us/step - loss: 0.0282 - accuracy: 0.9859 - val_loss: 0.0436 - val_accuracy: 0.9794\n",
      "Epoch 43/50\n",
      "67/67 [==============================] - 0s 838us/step - loss: 0.0268 - accuracy: 0.9887 - val_loss: 0.0312 - val_accuracy: 0.9850\n",
      "Epoch 44/50\n",
      "67/67 [==============================] - 0s 805us/step - loss: 0.0380 - accuracy: 0.9841 - val_loss: 0.0392 - val_accuracy: 0.9813\n",
      "Epoch 45/50\n",
      "67/67 [==============================] - 0s 834us/step - loss: 0.0343 - accuracy: 0.9869 - val_loss: 0.0591 - val_accuracy: 0.9738\n",
      "Epoch 46/50\n",
      "67/67 [==============================] - 0s 816us/step - loss: 0.0344 - accuracy: 0.9850 - val_loss: 0.0720 - val_accuracy: 0.9719\n",
      "Epoch 47/50\n",
      "67/67 [==============================] - 0s 834us/step - loss: 0.0321 - accuracy: 0.9855 - val_loss: 0.0248 - val_accuracy: 0.9925\n",
      "Epoch 48/50\n",
      "67/67 [==============================] - 0s 817us/step - loss: 0.0252 - accuracy: 0.9883 - val_loss: 0.0274 - val_accuracy: 0.9925\n",
      "Epoch 49/50\n",
      "67/67 [==============================] - 0s 822us/step - loss: 0.0271 - accuracy: 0.9850 - val_loss: 0.0521 - val_accuracy: 0.9719\n",
      "Epoch 50/50\n",
      "67/67 [==============================] - 0s 821us/step - loss: 0.0222 - accuracy: 0.9887 - val_loss: 0.0123 - val_accuracy: 0.9944\n",
      "Best epoch: 34\n"
     ]
    }
   ],
   "source": [
    "# Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(X_train_oversampled, y_train_oversampled, epochs=50, validation_split=0.2)\n",
    "\n",
    "val_acc_per_epoch = history.history['val_accuracy']\n",
    "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "print('Best epoch: %d' % (best_epoch,))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.921437 using {'learn_rate': 0.3, 'momentum': 0.4}\n",
      "0.888763 (0.059324) with: {'learn_rate': 0.001, 'momentum': 0.0}\n",
      "0.899335 (0.043818) with: {'learn_rate': 0.001, 'momentum': 0.2}\n",
      "0.895822 (0.049798) with: {'learn_rate': 0.001, 'momentum': 0.4}\n",
      "0.905517 (0.034849) with: {'learn_rate': 0.001, 'momentum': 0.6}\n",
      "0.871095 (0.057871) with: {'learn_rate': 0.001, 'momentum': 0.8}\n",
      "0.906430 (0.055439) with: {'learn_rate': 0.001, 'momentum': 0.9}\n",
      "0.806581 (0.055063) with: {'learn_rate': 0.01, 'momentum': 0.0}\n",
      "0.869283 (0.053607) with: {'learn_rate': 0.01, 'momentum': 0.2}\n",
      "0.884363 (0.070057) with: {'learn_rate': 0.01, 'momentum': 0.4}\n",
      "0.895810 (0.042154) with: {'learn_rate': 0.01, 'momentum': 0.6}\n",
      "0.856930 (0.047736) with: {'learn_rate': 0.01, 'momentum': 0.8}\n",
      "0.895820 (0.053047) with: {'learn_rate': 0.01, 'momentum': 0.9}\n",
      "0.810958 (0.100750) with: {'learn_rate': 0.1, 'momentum': 0.0}\n",
      "0.907288 (0.042124) with: {'learn_rate': 0.1, 'momentum': 0.2}\n",
      "0.895822 (0.049798) with: {'learn_rate': 0.1, 'momentum': 0.4}\n",
      "0.898495 (0.068832) with: {'learn_rate': 0.1, 'momentum': 0.6}\n",
      "0.888751 (0.048746) with: {'learn_rate': 0.1, 'momentum': 0.8}\n",
      "0.862184 (0.061995) with: {'learn_rate': 0.1, 'momentum': 0.9}\n",
      "0.898481 (0.057454) with: {'learn_rate': 0.2, 'momentum': 0.0}\n",
      "0.890522 (0.050539) with: {'learn_rate': 0.2, 'momentum': 0.2}\n",
      "0.887871 (0.052045) with: {'learn_rate': 0.2, 'momentum': 0.4}\n",
      "0.891392 (0.040377) with: {'learn_rate': 0.2, 'momentum': 0.6}\n",
      "0.850757 (0.055381) with: {'learn_rate': 0.2, 'momentum': 0.8}\n",
      "0.868450 (0.060616) with: {'learn_rate': 0.2, 'momentum': 0.9}\n",
      "0.835724 (0.073699) with: {'learn_rate': 0.3, 'momentum': 0.0}\n",
      "0.894973 (0.077379) with: {'learn_rate': 0.3, 'momentum': 0.2}\n",
      "0.921437 (0.047777) with: {'learn_rate': 0.3, 'momentum': 0.4}\n",
      "0.866640 (0.025992) with: {'learn_rate': 0.3, 'momentum': 0.6}\n",
      "0.878124 (0.045108) with: {'learn_rate': 0.3, 'momentum': 0.8}\n",
      "0.879890 (0.038755) with: {'learn_rate': 0.3, 'momentum': 0.9}\n"
     ]
    }
   ],
   "source": [
    "# Use scikit-learn to grid search the learning rate and momentum\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# from tf.keras.optimizers import SGD\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(learn_rate=0.01, momentum=0):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(6, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "X = X_train_oversampled\n",
    "Y = y_train_oversampled\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)\n",
    "# define the grid search parameters\n",
    "learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "param_grid = dict(learn_rate=learn_rate, momentum=momentum)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X, Y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brennanszeto/Environments/dsvenv/lib/python3.8/site-packages/sklearn/model_selection/_search.py:278: UserWarning: The total space of parameters 18 is smaller than n_iter=100. Running 18 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   30.0s\n",
      "[Parallel(n_jobs=-1)]: Done  54 out of  54 | elapsed:   46.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.907304 using {'epochs': 100, 'batch_size': 10}\n",
      "0.681946 (0.149657) with: {'epochs': 10, 'batch_size': 10}\n",
      "0.779205 (0.102228) with: {'epochs': 50, 'batch_size': 10}\n",
      "0.907304 (0.048757) with: {'epochs': 100, 'batch_size': 10}\n",
      "0.682802 (0.137671) with: {'epochs': 10, 'batch_size': 20}\n",
      "0.713806 (0.126166) with: {'epochs': 50, 'batch_size': 20}\n",
      "0.861342 (0.037683) with: {'epochs': 100, 'batch_size': 20}\n",
      "0.580539 (0.135932) with: {'epochs': 10, 'batch_size': 40}\n",
      "0.697875 (0.104039) with: {'epochs': 50, 'batch_size': 40}\n",
      "0.756200 (0.103668) with: {'epochs': 100, 'batch_size': 40}\n",
      "0.559254 (0.146420) with: {'epochs': 10, 'batch_size': 60}\n",
      "0.708457 (0.109958) with: {'epochs': 50, 'batch_size': 60}\n",
      "0.782716 (0.076441) with: {'epochs': 100, 'batch_size': 60}\n",
      "0.469243 (0.215609) with: {'epochs': 10, 'batch_size': 80}\n",
      "0.691688 (0.140931) with: {'epochs': 50, 'batch_size': 80}\n",
      "0.724325 (0.139656) with: {'epochs': 100, 'batch_size': 80}\n",
      "0.620905 (0.164163) with: {'epochs': 10, 'batch_size': 100}\n",
      "0.649297 (0.137553) with: {'epochs': 50, 'batch_size': 100}\n",
      "0.691641 (0.170397) with: {'epochs': 100, 'batch_size': 100}\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(6, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load dataset\n",
    "X = new_train_df.drop(columns=['bidder_id', 'payment_account', 'address', 'outcome','merchandise'])\n",
    "y = new_train_df['outcome']\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "sm = SMOTE()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.7, random_state=42)\n",
    "X_train_oversampled, y_train_oversampled = sm.fit_resample(X_train, y_train)\n",
    "# split into input (X) and output (Y) variables\n",
    "X = X_train_oversampled\n",
    "Y = y_train_oversampled\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "# define the grid search parameters\n",
    "batch_size = [10, 20, 40, 60, 80, 100]\n",
    "epochs = [10, 50, 100]\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "# random = RandomizedSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "random = RandomizedSearchCV(estimator = model, param_distributions = param_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "random_result = random.fit(X, Y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (random_result.best_score_, random_result.best_params_))\n",
    "means = random_result.cv_results_['mean_test_score']\n",
    "stds = random_result.cv_results_['std_test_score']\n",
    "params = random_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with autoML libraries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TPOT pipeline creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier\n",
    "pipeline_optimizer = TPOTClassifier()\n",
    "pipeline_optimizer = TPOTClassifier(generations=5, population_size=20, cv=5,\n",
    "                                    random_state=42, verbosity=2)\n",
    "pipeline_optimizer.fit(X_train, y_train)\n",
    "print(pipeline_optimizer.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyCaret classifier creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.datasets import get_data\n",
    "data = new_train_df\n",
    "from pycaret.classification import *\n",
    "exp_name = setup(data = new_train_df,target = 'outcome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = compare_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pycaret.datasets import get_data\n",
    "\n",
    "\n",
    "from pycaret.classification import *\n",
    "\n",
    "exp_name = setup(data = data,  target = 'outcome',silent=True)\n",
    "\n",
    "cb = create_model('catboost')\n",
    "\n",
    "optimize_threshold(cb, true_negative = 10, false_negative = -100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
